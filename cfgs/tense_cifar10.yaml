session: {
  _comment_: exp1,
  save_path: ./logs,
  project_name : CIFAR10,
}

# Support sklearn metrics or custom metrics
train: {
  num_epochs: 100,
  metrics: ["accuracy_score", "f1_score"],
}

# if you want to auto split please set
data : {
  data.class: data_loader.Classify.ClassificationDataset,
  validation_ratio: 0,
  data_csv_name: ./data/csv6labels_partial/data_train.csv,
  validation_csv_name : ./data/csv6labels_partial/data_val.csv,
  test_csv_name: ./data/csv6labels_partial/data_test.csv,
  label_dict: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], # sort by class_id
  batch_size: 128,
}

model_teacher: {
  model.class: models.efficientnet_transfer.efficientnet.Efficientnet_transfer,
  model_name: efficientnet_b5,
  pretrained: false,
  num_classes: 10,
}

model_robust: {
  model.class: models.resnet_transfer.resnet.Resnet_transfer,
  model_name: resnet50,
  pretrained: true,
  num_classes: 10,
}

loss: {
  name: LBGATLoss,
  weight: null,
  beta: 1,
}

adversarial: {
  perturb_steps: 10,
  step_size: 0.007,
  epsilon: 0.031,
  norm: np.inf,
}

# configure arguments of torch.optim.Optimizer
# You can add other arguments for the Optimizer
# (ex. "name": SGD)
optimizer: {
  name: SGD,
  lr: 1e-2,
  momentum: 0.9,
  weight_decay: 2e-4,
}

# configure arguments of torch.optim.lr_scheduler 
# feel free to add other arguments for the sheduler
# Do not modify this key (eg. "name": CosineAnnealingLR)
scheduler: {
  name: ReduceLROnPlateau,
  min_lr: 1e-6,
  mode: min,
  patience: 10,
  factor: 0.1,
}
